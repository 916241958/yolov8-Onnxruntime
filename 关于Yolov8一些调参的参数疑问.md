### 关于Yolov8一些调参的参数疑问

#### 部分超参数/设置含义

`lr0`：初始学习率，确定每次迭代的步长，同时使loss function达到最小值(个人建议，在你还不知道模型的拟合能力的时候建议设置较大学习率，使其能快速下降)。

`lrf`：最终学习率，占初始学习率的百分比=(lr0 * lrf)，与调度程序结合使用，随着时间的推移会逐步调整学习率。

`momentum`：SGD优化器的动量 / Adam优化器的beta1（可以将其视为让下降具有一个加速度）。

`weight_decay`：优化器的权重衰减，损失函数中的L2的正则化项（平方和作为正则项）作为惩罚项，用于防止过拟合（<font color='yellow'> PS：</font>个人觉得与其用正则化不如用好一点的数据集）。

`warmup_epochs`：热身阶段（初始阶段一部分）的轮数。在热身阶段内，学习率会按照从低到高逐渐增加到初始学习率(`lr0`)，以便在早期阶段稳定训练。

`warmup_bias_lr`：热身阶段开始时的偏置学习率初值。有助于稳定初始epochs的模型训练。

`box`：损失函数中边框损失部分的权重。

`cls`：损失函数中分类损失部分的权重。

`dfl`：DFL（Dynamic Freezing Loss）损失权重。解决类别不平衡问题（一些类出现频率低而一些类出现频率高）。

`pose`：姿态损失在姿态估计模型中的权重。

`kobj`：关键点对象性损失在姿态估计模型中的权重。

`label_smoothing`：标签平滑，正则化技术，用于减少模型对训练数据的过拟合程度。将hard label转变成soft label，相当于往真实分布中加入了噪声，防止过拟合。

`batch`：训练的批量大小，更新模型内部参数之前，前向传递中同时处理的图像数量，-1则为动态调整（不建议，会自动填充到你的显存所能容忍的最大值。建议在显存容忍的情况下从16，32.....这种方式手动测试）。

`epochs`：一个`epoch`是所有训练实例的一次完整的正向和反向传递。

`patience`：在验证指标没有改善的情况下，提前停止训练所需的`epoch`。当性能趋于平稳时停止训练，有助于防止过度拟合。

`optimizer`:优化器，可选择 SGD, Adam, AdamW, NAdam, RAdam, RMSProp 等，或auto，单引号赋值（自己的经验：`SGD`和`cos_lr`适合用于目标检测）。

`seed`：随机种子默认为0。设置种子方便复现结果。

`deterministic`：强制确定性算法，保证可重复性，默认为True。

`rect`：矩形训练，提高效率，默认为Fasle。

`cos_lr`：训练学习率衰减策略，余弦True/线性Flase。

`close_mosaic`：在训练完成前最后 N 个epoch禁用mosaic数据增强（始于yolov4）以稳定训练。

`amp`：启用自动混合精度 (AMP) 训练，可减少内存使用量并加快训练速度，同时将对精度的影响降至最低，默认为True。（在没有amp之前基本都是`TP32`进行训练，17之后英伟达提出了`TP16`和`TP32`混合训练，结果和`TP32`相差不大）

`plots`：控制训练图和验证图的生成和保存。设置为 True 创建损失曲线、精度-召回曲线和样本预测等图表。有助于直观地跟踪模型随时间变化的性能。

